{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Temple HSC.ipynb","provenance":[],"authorship_tag":"ABX9TyOv+l6IaRXv5FaQEI5C7s1A"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"code","metadata":{"id":"_MJq9Z8eot17"},"source":["from google.colab import drive \n","drive.mount('/content/drive')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"HkwmmSjjowsW","executionInfo":{"status":"ok","timestamp":1613643378869,"user_tz":-210,"elapsed":33579,"user":{"displayName":"ali garjani","photoUrl":"","userId":"14880780470972426711"}}},"source":["import os\n","os.chdir('drive/My Drive/R/Tempel-HSC-')"],"execution_count":2,"outputs":[]},{"cell_type":"code","metadata":{"id":"X5Jz3Ik9pBeJ"},"source":["from torch.utils.data import DataLoader\n","from model import Classifier\n","from src.models.models import AttentionModel\n","from src.scripts.create_dataset import create_dataset\n","from loader import load_datasets\n","import os\n","import pandas as pd"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"hn_I_17kpHuN","executionInfo":{"status":"ok","timestamp":1613643606192,"user_tz":-210,"elapsed":824,"user":{"displayName":"ali garjani","photoUrl":"","userId":"14880780470972426711"}}},"source":["PATHS = {'train': './data/processed/{}_T{}_{}/{}/triplet_dbscan_train.csv',\n","         'test': './data/processed/{}_T{}_{}/{}/triplet_dbscan_test.csv',\n","         'result': './results/{}_T{}_{}'}\n","\n","parameters = {\n","    'hidden_size': 128,\n","\n","    'dropout_p': 0.5,\n","\n","    'learning_rate': 0.001,\n","\n","    'batch_size': 256,\n","\n","    'input_dim': 100,\n","\n","    'num_of_epochs': 50,\n","\n","    'lr_milestones': [25]\n","}\n","\n","dataset_features = {\n","    'dataset': 'H5N1',\n","\n","    'num_of_datasets': 5,\n","\n","    'start_year': 2001,\n","\n","    'end_year': 2016,\n","}\n","create_dataset_cmd = False\n","train_cmd = True"],"execution_count":7,"outputs":[]},{"cell_type":"code","metadata":{"id":"BIVDnlHIo__y"},"source":["if create_dataset_cmd:\n","    for i in range(dataset_features['num_of_datasets']):\n","        create_dataset(dataset_features['start_year'], dataset_features['end_year'], dataset_features['dataset'], i + 1)\n","if train_cmd:\n","    res_path = PATHS['result'].format(dataset_features['dataset'],\n","                                      dataset_features['end_year'] -\n","                                      dataset_features['start_year'],\n","                                      dataset_features['end_year'])\n","    if not os.path.exists(res_path):\n","        os.mkdir(res_path)\n","    final_res = {'mean': {'precision': 0, 'recall': 0, 'f-score': 0, 'mcc': 0, 'accuracy': 0, 'auc': 0},\n","                'var': {'precision': 0, 'recall': 0, 'f-score': 0, 'mcc': 0, 'accuracy': 0, 'auc': 0}}\n","    for i in range(dataset_features['num_of_datasets']):\n","        train_dataset, valid_dataset, test_dataset = load_datasets(dataset_features['dataset'],\n","                                                                    PATHS['train'].format(dataset_features['dataset'],\n","                                                                                          dataset_features['end_year'] -\n","                                                                                          dataset_features['start_year'],\n","                                                                                          dataset_features['end_year'],\n","                                                                                          i + 1),\n","                                                                    PATHS['test'].format(dataset_features['dataset'],\n","                                                                                         dataset_features['end_year'] -\n","                                                                                         dataset_features['start_year'],\n","                                                                                         dataset_features['end_year'],\n","                                                                                         i + 1))\n","\n","        train_loader = DataLoader(\n","            dataset=train_dataset,\n","            batch_size=parameters['batch_size'], shuffle=True, drop_last=True)\n","        val_loader = DataLoader(\n","            dataset=valid_dataset,\n","            batch_size=parameters['batch_size'], shuffle=False, drop_last=False)\n","        test_loader = DataLoader(\n","            dataset=test_dataset,\n","            batch_size=parameters['batch_size'], shuffle=False, drop_last=False)\n","\n","        seq_length = dataset_features['end_year'] - dataset_features['start_year'] - 1\n","        net = AttentionModel(seq_length, parameters['input_dim'], parameters['hidden_size']\n","                              , parameters['dropout_p']).float()\n","        classifier = Classifier(batch_size=parameters['batch_size'], lr_milestones=parameters['lr_milestones']\n","                                , n_epochs=parameters['num_of_epochs'])\n","        classifier.train(net, train_loader, test_loader, val_loader)\n","\n","        df = pd.DataFrame.from_dict(classifier.scores)\n","        df.to_csv(res_path + '/{}.csv'.format(i + 1))\n","        for k, v in classifier.scores['test'].items():\n","            final_res['mean'][k] = (final_res['mean'][k] * i + sum(v) / len(v)) / (i + 1)\n","        for k, v in classifier.scores['test'].items():\n","            final_res['var'][k] = (i * (final_res['mean'][k] - sum(v) / len(v)) ** 2 + final_res['var'][k]) / (i + 1)\n","    df = pd.DataFrame.from_dict(final_res)\n","    df.to_csv(res_path + '/final.csv')\n"],"execution_count":null,"outputs":[]}]}