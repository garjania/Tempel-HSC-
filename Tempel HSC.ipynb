{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1936,
     "status": "ok",
     "timestamp": 1614179962873,
     "user": {
      "displayName": "Ali Garjani",
      "photoUrl": "",
      "userId": "01496906712636820898"
     },
     "user_tz": -210
    },
    "id": "X5Jz3Ik9pBeJ",
    "outputId": "173ed24d-7276-41c5-ce62-bb932638d5ef"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/utils/deprecation.py:144: FutureWarning: The sklearn.neighbors.nearest_centroid module is  deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.neighbors. Anything that cannot be imported from sklearn.neighbors is now part of the private API.\n",
      "  warnings.warn(message, FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from model import Classifier\n",
    "from src.models.models import AttentionModel\n",
    "from src.scripts.create_dataset import create_dataset\n",
    "from loader import load_datasets\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "executionInfo": {
     "elapsed": 1932,
     "status": "ok",
     "timestamp": 1614179962874,
     "user": {
      "displayName": "Ali Garjani",
      "photoUrl": "",
      "userId": "01496906712636820898"
     },
     "user_tz": -210
    },
    "id": "hn_I_17kpHuN",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "parameters = {\n",
    "    'hidden_size': 128,\n",
    "\n",
    "    'dropout_p': 0.5,\n",
    "\n",
    "    'learning_rate': 0.001,\n",
    "\n",
    "    'batch_size': 256,\n",
    "\n",
    "    'input_dim': 100,\n",
    "\n",
    "    'num_of_epochs': 50,\n",
    "\n",
    "    'lr_milestones': [25],\n",
    "}\n",
    "\n",
    "dataset_features = {\n",
    "    'dataset': 'H3N2',\n",
    "\n",
    "    'num_of_runs': 5,\n",
    "\n",
    "    'start_year': 2001,\n",
    "\n",
    "    'end_year': 2016,\n",
    "\n",
    "    'method': 'dbscan'\n",
    "}\n",
    "create_dataset_cmd = True\n",
    "train_cmd = False\n",
    "\n",
    "PATHS = {'train': './data/processed/{}_T{}_{}/{}/triplet_' + dataset_features['method'] + '_train.csv',\n",
    "         'test': './data/processed/{}_T{}_{}/{}/triplet_' + dataset_features['method'] + '_test.csv',\n",
    "         'result': './results/Tempel-HSC/{}_T{}_{}'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "if create_dataset_cmd:\n",
    "  for i in range(dataset_features['num_of_runs']):\n",
    "    create_dataset(dataset_features['start_year'], dataset_features['end_year'], dataset_features['dataset'], i + 1, method=dataset_features['method'])\n",
    "\n",
    "if train_cmd:\n",
    "    res_path = PATHS['result'].format(dataset_features['dataset'],\n",
    "                                      dataset_features['end_year'] -\n",
    "                                      dataset_features['start_year'],\n",
    "                                      dataset_features['end_year'])\n",
    "    if not os.path.exists(res_path):\n",
    "        os.makedirs(res_path)\n",
    "    final_res = {'mean': {'precision': 0, 'recall': 0, 'f-score': 0, 'mcc': 0, 'accuracy': 0, 'auc': 0},\n",
    "                'var': {'precision': 0, 'recall': 0, 'f-score': 0, 'mcc': 0, 'accuracy': 0, 'auc': 0}}\n",
    "    for i in range(dataset_features['num_of_runs']):\n",
    "        train_dataset, valid_dataset, test_dataset = load_datasets(dataset_features['dataset'],\n",
    "                                                                    PATHS['train'].format(dataset_features['dataset'],\n",
    "                                                                                          dataset_features['end_year'] -\n",
    "                                                                                          dataset_features['start_year'],\n",
    "                                                                                          dataset_features['end_year'],\n",
    "                                                                                          i + 1),\n",
    "                                                                    PATHS['test'].format(dataset_features['dataset'],\n",
    "                                                                                         dataset_features['end_year'] -\n",
    "                                                                                         dataset_features['start_year'],\n",
    "                                                                                         dataset_features['end_year'],\n",
    "                                                                                         i + 1))\n",
    "\n",
    "        train_loader = DataLoader(\n",
    "            dataset=train_dataset,\n",
    "            batch_size=parameters['batch_size'], shuffle=True, drop_last=True)\n",
    "        val_loader = DataLoader(\n",
    "            dataset=valid_dataset,\n",
    "            batch_size=parameters['batch_size'], shuffle=False, drop_last=False)\n",
    "        test_loader = DataLoader(\n",
    "            dataset=test_dataset,\n",
    "            batch_size=parameters['batch_size'], shuffle=False, drop_last=False)\n",
    "\n",
    "        seq_length = dataset_features['end_year'] - dataset_features['start_year']\n",
    "        net = AttentionModel(seq_length, parameters['input_dim'], 2, parameters['hidden_size']\n",
    "                                , parameters['dropout_p']).float()\n",
    "\n",
    "        classifier = Classifier(batch_size=parameters['batch_size'], lr_milestones=parameters['lr_milestones']\n",
    "                                , n_epochs=parameters['num_of_epochs'])\n",
    "        classifier.train(net, train_loader, test_loader, val_loader)\n",
    "\n",
    "        df = pd.DataFrame.from_dict(classifier.scores)\n",
    "        df.to_csv(res_path + '/{}.csv'.format(i + 1))\n",
    "        for k, v in classifier.scores['test'].items():\n",
    "            final_res['mean'][k] = (final_res['mean'][k] * i + sum(v) / len(v)) / (i + 1)\n",
    "        for k, v in classifier.scores['test'].items():\n",
    "            final_res['var'][k] = (i * (final_res['mean'][k] - sum(v) / len(v)) ** 2 + final_res['var'][k]) / (i + 1)\n",
    "\n",
    "    df = pd.DataFrame.from_dict(final_res)\n",
    "    df.to_csv(res_path + '/final.csv')\n",
    "    np.save(res_path + '/fpr', classifier.roc_info['fpr'])\n",
    "    np.save(res_path + '/tpr', classifier.roc_info['tpr'])\n",
    "    np.save(res_path + '/thresh', classifier.roc_info['thresh'])\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Temple HSC.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}